\section{Quality Metrics}
\label{sect:quality-metrics}

We start by looking at common quality metrics of cartograms, which are closely related to the problem we are trying to solve in this thesis as previously discussed in \cref{sect:related-work}, and discussing how they translate to the visualizations our framework creates.
We then discuss different ideas to formalize and quantify the previously mentioned notion of local fatness of the regions.



\paragraph{Quality Metrics of Cartograms}

Three well-established quantifiable measures are commonly used to assess a cartograms's quality \cite{alam2015quantitative} \cite{nusrat2018evaluating}:

\begin{itemize}
\item \textbf{Statistical accuracy:}
The statistical accuracy of a cartogram describes how closely the areas of the modified geographic regions match the variable of interest.
Recall that the normalized cartographic error of a region $v$ is defined as $\frac{\abs{A^\prime(v)-w(v)}}{max(A^\prime(v),w(v))}$ where $A^\prime(v)$ is its normalized actual area and $w(v)$ is its desired area.
The maximum and average normalized cartographic error over all regions in the cartogram are commonly used to quantify its statistical accuracy.
We borrow this quality metric from cartograms and use it for our visualizations in the form of $\varepsilon$-area-proportionality.

\item \textbf{Topological accuracy:}
The topological accuracy describes how well the original adjacencies between the geographic regions are preserved in the cartogram.
Considering our framework computes a polygonal contact representation of the filtered cluster graph, those and only those regions whose corresponding vertices are adjacent in the cluster graph are adjacent in the contact representation.
In terms of topological accuracy, we therefore produce perfect drawings.

\item \textbf{Geographic accuracy:}
The geographic accuracy of a cartogram describes to what degree the shapes and positions of the distorted regions resemble their original counterpart on the real geographic map.
In our case, however, there is no real geographic map that our visualization is based upon:
The maps we generate are entirely artificial and there is no geographic reference map.

But why is this resemblance a meaningful quality metric?
Geographic accuracy captures the preservation of the viewer's mental map between the real geographic map and the distorted map in the cartogram.
And this does apply to our framework as well, albeit only once we start incorporating dynamic updates into the artificial map.
Our dynamic pipeline naturally preserves the viewer's mental map by only allowing small, incremental changes to be incorporated and redrawing the artificial map using a force-directed algorithm.
This makes it easy for the viewer to track how the map changes between versions of the underlying cluster graph at different points in time.
\end{itemize}



\paragraph{Polygon Complexity}

Brinkhoff \etal{} \cite{brinkhoff1995measuring} propose a method to quantify the complexity of polygons.
They measure the complexity of polygons as a combination of three properties: the frequency of the vibration of its boundary, the amplitude of said vibration, and the deviation from its convex hull.
Given a polygon $P$, they count the number of corners where the polygon's internal angle is more than $180^\circ$, called notches, and define the fraction of those corners as
%
\begin{equation*}
\text{notches}_{\lbrack0,1\rbrack}(P) \coloneqq
\begin{cases}
\frac{\text{notches}(P)}{n - 3} & \text{if $n$ > 3}\\
0 & \text{otherwise}
\end{cases}
\in \lbrack0,1\rbrack
,
\end{equation*}
%
where $n$ is the total number of corners in $P$ and $\text{notches}(P)$ the number of corners that are notches.
They use this fraction to define the frequency of the vibration as
%
\begin{equation*}
\text{freq}(P) \coloneqq 1
+ 16 \cdot (\text{notches}_{\lbrack0,1\rbrack}(P) - 0.5)^4
- 8 \cdot (\text{notches}_{\lbrack0,1\rbrack}(P) - 0.5)^2
\in \lbrack0,1\rbrack
\end{equation*}
%
and its amplitude as
\begin{equation*}
\text{ampl}(P) \coloneqq
\frac{\text{circumference}(P) - \text{circumference}(\text{hull}(P))}{\text{circumference}(P)}
\in \lbrack0,1\rbrack
,
\end{equation*}
%
where $\text{circumference}(P)$ is the the length of $P$'s boundary and $\text{hull}(P)$ is $P$'s convex hull in the form of another polygon.
For convex polygons $P$, both the frequency $\text{freq}(P)$ and the amplitude $\text{ampl}(P)$ of the vibration of its boundary is zero.
The frequency value is at its maximum when half of the polygon's corners are notches.

To measure the convexity of a polygon, Brinkhoff \etal{} use the fraction of the area of the polygon's convex hull that the polygon itself covers:
%
\begin{equation*}
\text{conv}(P) \coloneqq
\frac{\text{area}(\text{hull}(P)) - \text{area}(P)}{\text{area}(\text{hull}(P))}
\in \lbrack0,1\rbrack
\end{equation*}

They then combine these three properties as
\begin{equation}
\text{compl}(P) \coloneqq
0.8 \cdot \text{ampl}(P) \cdot \text{freq}(P) + 0.2 \cdot \text{conv}(P)
\in \lbrack0,1\rbrack
\end{equation}
%
to measure the overall complexity of a polygon.

We adopt this definition of polygon complexity as a quality metric for the maps generated by our framework, but only with a slight change because as is, $\text{compl}(\cdot)$ assigns a perfect score of $0$ to all convex polygons.
This means that, for example, there is no distinction between a square region and a long, drawn-out rectangular region, even though the square regions matches our understanding of local fatness much better.
Instead of calculating the fraction of area a polygon $P$ doesn't cover within its convex hull, we therefore compute the fraction of area it doesn't cover in its smallest enclosing circle.
We normalize this fraction to unit interval by dividing by whatever area a maximal regular $n$-gon covers in a circle of that size.
In our tests, this measure has shown to align with our intuitive understanding of local fatness nicely.



\paragraph{Entropy}

Chen and Sundaram \cite{chen2005estimating} studied the complexity of 2-dimensional shapes in the field of computer vision.
The shapes they dealt with are given in the form of a point cloud.
For each point, they compute the Euclidean distance to the point cloud's centroid.
They also implement a heuristic to predict the contour of the shape at each point and use it to compute a local angle for each of the points.
The distances and local angles are then plotted into a histogram with flexible bin size and are used to compute the entropy of the distribution of distances and local angles.

In our use case, we don't even have to implement the heuristic to guess the shape's contour.
We know that the points form a closed curve that is in fact a polygon.
However, by connecting the points in this predetermined manner, we possibly get drastically different edge lengths.
The points on these edges that aren't corners of the polygon have no impact on the computed entropies at all.
This problem becomes obvious when we consider two straight-line segments of the same length, one that is a side of our polygon, and one which is the union of multiple sides of our polygon with corners (with internal angle $180^\circ$) in between:
The one with additional corners has a much greater impact on the computed entropies, even though both look the same to an observer.

We can try to remedy by subdividing the polygon's sides first, creating roughly uniform side lengths.
However, in doing so, we create lots of corners at which the polygon has an internal angle of $180^\circ$, quickly dominating and distorting the entropy of internal angles.

In our tests, neither of two entropy-based measures, with or without our adjustments, captured our intuitive understanding of local fatness satisfactorily.
We therefore conduct our experimental evaluation in \cref{sect:experimental-evaluation} using just cartographic error and our modified version of polygonal complexity from Brinkhoff \etal{} \cite{brinkhoff1995measuring} discussed above.
For both measures, we will look at both the maximum and average value over all regions in the map.
